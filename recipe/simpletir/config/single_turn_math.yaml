hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  val_sample_size: null
  apply_chat_template: True
  filter_overlong_prompts: True
  max_start_length: 4096
  max_obs_length: 256
  prompt: |
    Please reason step by step using comments and output your answer with `final_answer(answer)` within ```py and ```<end_code>.

    For example:
    Task: What is the result of the following operation: 5 + 3 + 1294.678?
    Assistant:
    ```py
    # Compute the result of the operation.
    answer = 5 + 3 + 1294.678

    # Return the final answer.
    final_answer(answer)
    ```<end_code>

    Now start:
    Task: 

actor_rollout_ref:
  actor:
    optim:
      lr: 1e-6
    use_dynamic_bsz: True
    use_kl_loss: False
    kl_loss_coef: 0.0
    kl_loss_type: low_var_kl
    entropy_coeff: 0.0
    clip_ratio_high: 0.28
    clip_ratio_low: 0.2
    clip_ratio_c: 3.0
    mask_tool_output: True
    mask_void_turns: True
  model:
    use_remove_padding: True
  rollout:
    temperature: 1.0
    min_p: 0.0
    swap_space: 40
    model_path: ${actor_rollout_ref.model.path}
    disable_log_stats: False
    enforce_eager: False
    free_cache_engine: False
    n: 16
    min_n: 4 # at least 4 responses should be valid for each prompt, otherwise we will skip it. Invalid ones include those which are over long.
    val_kwargs:
      # sampling parameters for validation
      top_k: -1 # 0 for hf rollout, -1 for vllm rollout
      top_p: 0.7
      temperature: 1.0
      n: 4
      do_sample: True # default eager for validation
      k: ${actor_rollout_ref.rollout.val_kwargs.n} # pass@k, 1 <= k_val <= actor_rollout_ref.rollout.n_val, default actor_rollout_ref.rollout.n_val
      detokenize: true
      stop: </code>
    ref:
      fsdp_config:
        param_offload: True

trainer:
  rejection_sample: True
  oversample_multiplier: 2.0 # Multiple the training batch size by this factor to account for rejection sampling reduction.
  remove_clip: False # remove overlong response if True
  acc_filter: False # if True, only keep prompts with avg acc ratio in thresholds
  acc_filter_high: 1
  acc_filter_low: 0
  start_clip_step: 50
  critic_warmup: 0
  logger: ['console','wandb']

critic:
  ppo_micro_batch_size_per_gpu: 4

reward_model:
  reward_manager: math_exec

algorithm:
  use_kl_in_reward: False
  kl_ctrl:
    kl_coef: 0.0

agent:
  tool_use: False
  max_turns: 1
  append_final_answer_func: False
